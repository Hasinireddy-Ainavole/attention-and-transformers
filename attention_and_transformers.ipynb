{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBM2TjynfmtSGcMPzd1y6/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hasinireddy-Ainavole/attention-and-transformers/blob/main/attention_and_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoLZQyqJpD5Q",
        "outputId": "24d12f31-88c0-4861-bc15-e8bdd5dca371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shapes:\n",
            "Q shape: (2, 4, 8)\n",
            "K shape: (2, 4, 8)\n",
            "V shape: (2, 4, 8)\n",
            "\n",
            "Output Shapes:\n",
            "Context shape: (2, 4, 8)\n",
            "Attention weights shape: (2, 4, 4)\n",
            "\n",
            "Attention Weights (first batch):\n",
            "[[0.50258161 0.07125796 0.29559999 0.13056044]\n",
            " [0.1356621  0.4668456  0.09868613 0.29880617]\n",
            " [0.04296256 0.73176166 0.07266276 0.15261301]\n",
            " [0.54250442 0.19083811 0.14071448 0.12594299]]\n",
            "\n",
            "Sum of attention weights (should be ~1.0 for each query):\n",
            "[1. 1. 1. 1.]\n",
            "\n",
            "Context Vector (first batch, first query):\n",
            "[-0.01769307 -0.01455707 -1.04874428 -0.53684339 -0.11980371  0.48081568\n",
            " -0.65812311  0.98025517]\n",
            "\n",
            "==================================================\n",
            "Example with Causal Masking:\n",
            "==================================================\n",
            "\n",
            "Causal Attention Weights (first batch):\n",
            "[[1.         0.         0.         0.        ]\n",
            " [0.22516244 0.77483756 0.         0.        ]\n",
            " [0.05070005 0.86355075 0.0857492  0.        ]\n",
            " [0.54250442 0.19083811 0.14071448 0.12594299]]\n",
            "\n",
            "Note: Upper triangle is zero (future positions masked)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Compute scaled dot-product attention.\n",
        "\n",
        "    Args:\n",
        "        Q: Query matrix of shape (batch_size, seq_len_q, d_k)\n",
        "        K: Key matrix of shape (batch_size, seq_len_k, d_k)\n",
        "        V: Value matrix of shape (batch_size, seq_len_v, d_v)\n",
        "        mask: Optional mask of shape (batch_size, seq_len_q, seq_len_k)\n",
        "\n",
        "    Returns:\n",
        "        context: Context vector of shape (batch_size, seq_len_q, d_v)\n",
        "        attention_weights: Attention weights of shape (batch_size, seq_len_q, seq_len_k)\n",
        "    \"\"\"\n",
        "    # Get the dimension of the key vectors\n",
        "    d_k = K.shape[-1]\n",
        "\n",
        "    # Step 1: Compute attention scores (Q · K^T)\n",
        "    # Shape: (batch_size, seq_len_q, seq_len_k)\n",
        "    scores = np.matmul(Q, K.transpose(0, 2, 1))\n",
        "\n",
        "    # Step 2: Scale by sqrt(d_k)\n",
        "    scaled_scores = scores / np.sqrt(d_k)\n",
        "\n",
        "    # Step 3: Apply mask if provided (optional)\n",
        "    if mask is not None:\n",
        "        scaled_scores = np.where(mask == 0, -1e9, scaled_scores)\n",
        "\n",
        "    # Step 4: Apply softmax to get attention weights\n",
        "    # Softmax is applied along the last dimension (over keys)\n",
        "    attention_weights = softmax(scaled_scores, axis=-1)\n",
        "\n",
        "    # Step 5: Compute context vector (weighted sum of values)\n",
        "    # Shape: (batch_size, seq_len_q, d_v)\n",
        "    context = np.matmul(attention_weights, V)\n",
        "\n",
        "    return context, attention_weights\n",
        "\n",
        "\n",
        "def softmax(x, axis=-1):\n",
        "    \"\"\"\n",
        "    Compute softmax values for array x along specified axis.\n",
        "    Uses numerical stability trick (subtract max).\n",
        "    \"\"\"\n",
        "    # Subtract max for numerical stability\n",
        "    x_max = np.max(x, axis=axis, keepdims=True)\n",
        "    exp_x = np.exp(x - x_max)\n",
        "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
        "\n",
        "\n",
        "# Example usage and testing\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seed for reproducibility\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Define dimensions\n",
        "    batch_size = 2\n",
        "    seq_len_q = 4  # Query sequence length\n",
        "    seq_len_k = 4  # Key sequence length (same as value)\n",
        "    d_k = 8        # Dimension of key/query vectors\n",
        "    d_v = 8        # Dimension of value vectors\n",
        "\n",
        "    # Create random query, key, value matrices\n",
        "    Q = np.random.randn(batch_size, seq_len_q, d_k)\n",
        "    K = np.random.randn(batch_size, seq_len_k, d_k)\n",
        "    V = np.random.randn(batch_size, seq_len_k, d_v)\n",
        "\n",
        "    print(\"Input Shapes:\")\n",
        "    print(f\"Q shape: {Q.shape}\")\n",
        "    print(f\"K shape: {K.shape}\")\n",
        "    print(f\"V shape: {V.shape}\")\n",
        "    print()\n",
        "\n",
        "    # Compute attention\n",
        "    context, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "    print(\"Output Shapes:\")\n",
        "    print(f\"Context shape: {context.shape}\")\n",
        "    print(f\"Attention weights shape: {attention_weights.shape}\")\n",
        "    print()\n",
        "\n",
        "    print(\"Attention Weights (first batch):\")\n",
        "    print(attention_weights[0])\n",
        "    print()\n",
        "\n",
        "    # Verify that attention weights sum to 1 along the last dimension\n",
        "    print(\"Sum of attention weights (should be ~1.0 for each query):\")\n",
        "    print(np.sum(attention_weights[0], axis=-1))\n",
        "    print()\n",
        "\n",
        "    print(\"Context Vector (first batch, first query):\")\n",
        "    print(context[0, 0])\n",
        "\n",
        "    # Example with masking (e.g., for causal attention)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Example with Causal Masking:\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Create causal mask (lower triangular matrix)\n",
        "    causal_mask = np.tril(np.ones((seq_len_q, seq_len_k)))\n",
        "    causal_mask = np.expand_dims(causal_mask, 0)  # Add batch dimension\n",
        "    causal_mask = np.repeat(causal_mask, batch_size, axis=0)\n",
        "\n",
        "    context_masked, attention_weights_masked = scaled_dot_product_attention(\n",
        "        Q, K, V, mask=causal_mask\n",
        "    )\n",
        "\n",
        "    print(\"\\nCausal Attention Weights (first batch):\")\n",
        "    print(attention_weights_masked[0])\n",
        "    print(\"\\nNote: Upper triangle is zero (future positions masked)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-Head Self-Attention mechanism.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: Model dimension (embedding size)\n",
        "            num_heads: Number of attention heads\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads  # Dimension per head\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Output projection\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Compute scaled dot-product attention.\n",
        "\n",
        "        Args:\n",
        "            Q, K, V: Shape (batch_size, num_heads, seq_len, d_k)\n",
        "            mask: Optional mask\n",
        "        \"\"\"\n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Apply softmax\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Compute context\n",
        "        context = torch.matmul(attention_weights, V)\n",
        "\n",
        "        return context, attention_weights\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "            mask: Optional attention mask\n",
        "\n",
        "        Returns:\n",
        "            output: Shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "\n",
        "        # Linear projections and split into multiple heads\n",
        "        # Shape: (batch_size, seq_len, d_model) -> (batch_size, num_heads, seq_len, d_k)\n",
        "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Apply attention\n",
        "        context, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Concatenate heads\n",
        "        # Shape: (batch_size, num_heads, seq_len, d_k) -> (batch_size, seq_len, d_model)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
        "\n",
        "        # Final linear projection\n",
        "        output = self.W_o(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    \"\"\"Position-wise Feed-Forward Network.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: Model dimension\n",
        "            d_ff: Hidden dimension of feed-forward network\n",
        "            dropout: Dropout rate\n",
        "        \"\"\"\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "\n",
        "        Returns:\n",
        "            output: Shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"Single Transformer Encoder Block with Multi-Head Attention and FFN.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: Model dimension (embedding size)\n",
        "            num_heads: Number of attention heads\n",
        "            d_ff: Hidden dimension of feed-forward network\n",
        "            dropout: Dropout rate\n",
        "        \"\"\"\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "\n",
        "        # Multi-head self-attention\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout)\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "            mask: Optional attention mask\n",
        "\n",
        "        Returns:\n",
        "            output: Shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # Multi-head attention with residual connection and layer norm\n",
        "        attn_output = self.attention(x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))  # Add & Norm\n",
        "\n",
        "        # Feed-forward network with residual connection and layer norm\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_output))  # Add & Norm\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Example usage and testing\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Define dimensions as per the question\n",
        "    batch_size = 32\n",
        "    seq_len = 10\n",
        "    d_model = 512      # Model dimension\n",
        "    num_heads = 8      # Number of attention heads\n",
        "    d_ff = 2048        # Feed-forward hidden dimension\n",
        "    dropout = 0.1\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"Transformer Encoder Block - Configuration\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    print(f\"Sequence length: {seq_len}\")\n",
        "    print(f\"Model dimension (d_model): {d_model}\")\n",
        "    print(f\"Number of heads: {num_heads}\")\n",
        "    print(f\"Feed-forward dimension (d_ff): {d_ff}\")\n",
        "    print(f\"Dropout rate: {dropout}\")\n",
        "    print()\n",
        "\n",
        "    # Initialize the encoder block\n",
        "    encoder_block = TransformerEncoderBlock(d_model, num_heads, d_ff, dropout)\n",
        "\n",
        "    # Create sample input (batch of 32 sentences, each with 10 tokens)\n",
        "    x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "    print(\"Input shape:\", x.shape)\n",
        "    print()\n",
        "\n",
        "    # Forward pass\n",
        "    output = encoder_block(x)\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"Output Verification\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Expected shape: torch.Size([{batch_size}, {seq_len}, {d_model}])\")\n",
        "    print()\n",
        "\n",
        "    # Verify output shape\n",
        "    assert output.shape == (batch_size, seq_len, d_model), \"Output shape mismatch!\"\n",
        "    print(\"✓ Output shape verification passed!\")\n",
        "    print()\n",
        "\n",
        "    # Display model architecture\n",
        "    print(\"=\"*60)\n",
        "    print(\"Model Architecture Summary\")\n",
        "    print(\"=\"*60)\n",
        "    print(encoder_block)\n",
        "    print()\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in encoder_block.parameters())\n",
        "    trainable_params = sum(p.numel() for p in encoder_block.parameters() if p.requires_grad)\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"Parameter Count\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print()\n",
        "\n",
        "    # Test with evaluation mode\n",
        "    encoder_block.eval()\n",
        "    with torch.no_grad():\n",
        "        output_eval = encoder_block(x)\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"Evaluation Mode Test\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Evaluation output shape: {output_eval.shape}\")\n",
        "    print(\"✓ Evaluation mode test passed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMAyKNWKpfTE",
        "outputId": "f55f14bf-4c4d-4baa-e470-706e6f534273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Transformer Encoder Block - Configuration\n",
            "============================================================\n",
            "Batch size: 32\n",
            "Sequence length: 10\n",
            "Model dimension (d_model): 512\n",
            "Number of heads: 8\n",
            "Feed-forward dimension (d_ff): 2048\n",
            "Dropout rate: 0.1\n",
            "\n",
            "Input shape: torch.Size([32, 10, 512])\n",
            "\n",
            "============================================================\n",
            "Output Verification\n",
            "============================================================\n",
            "Output shape: torch.Size([32, 10, 512])\n",
            "Expected shape: torch.Size([32, 10, 512])\n",
            "\n",
            "✓ Output shape verification passed!\n",
            "\n",
            "============================================================\n",
            "Model Architecture Summary\n",
            "============================================================\n",
            "TransformerEncoderBlock(\n",
            "  (attention): MultiHeadAttention(\n",
            "    (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
            "  )\n",
            "  (ffn): FeedForwardNetwork(\n",
            "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "  (dropout2): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "\n",
            "============================================================\n",
            "Parameter Count\n",
            "============================================================\n",
            "Total parameters: 3,152,384\n",
            "Trainable parameters: 3,152,384\n",
            "\n",
            "============================================================\n",
            "Evaluation Mode Test\n",
            "============================================================\n",
            "Evaluation output shape: torch.Size([32, 10, 512])\n",
            "✓ Evaluation mode test passed!\n"
          ]
        }
      ]
    }
  ]
}